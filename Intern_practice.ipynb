{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d781d3-256c-405c-85da-cd50080380b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab0a93-cd2b-4b25-af24-6cbb2ee7bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # Install Open Cv - PIP - tool to install packages meaning PIP Install Packages/Python pip install opencv-python\n",
    "# pip install opencv-python\n",
    "# pip install deepface\n",
    "# pip install fer -- version 22.4.0 and moviepy 1.0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8cc39-5afb-45ac-91cc-3203c81da8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking if its installed correctly and understanding the version\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "import os\n",
    "\n",
    "from deepface import DeepFace\n",
    "import moviepy\n",
    "print(\"moviepy is working!\")\n",
    "\n",
    "# from fer import FER\n",
    "# print(fer.__version__)\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# print(deepface.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e9723-5ecf-4c26-87b7-5daf81841211",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/smile.mp4')\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"❌ Error opening video file.\")\n",
    "else:\n",
    "    print(\"✅ Video successfully loaded!\");\n",
    "    ret, frame = video.read()\n",
    "    print('ret', ret, 'frame' ,frame, video)\n",
    "    if ret:\n",
    "        print('✅ Frame extracted!');\n",
    "        cv2.imwrite('first_frame.jpg',frame)\n",
    "    else:\n",
    "        print(\"❌ Error reading video file.\")\n",
    "    \n",
    "video.release();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8521ff-84dc-4207-8348-ec291a60e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract all the frames in a folder \n",
    "# Always open video again - dont skip this step \n",
    "video = cv2.VideoCapture('/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/smile.mp4')\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"❌ Error opening video file.\")\n",
    "smile_frames =\"smile frames\"\n",
    "if not os.path.exists(smile_frames):\n",
    "    os.makedirs(smile_frames)\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "for frame_num in range (1, frame_count+1):\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print(\"❌ Error reading video file.\")\n",
    "        print(f\"Stopped at frame {frame_num}\")\n",
    "        break\n",
    "    frame_filename = os.path.join(smile_frames, f\"frame_{frame_num}.jpg\")\n",
    "    cv2.imwrite(frame_filename,frame)\n",
    "    print(f\"Saved {frame_filename}\")\n",
    "\n",
    "# video.release()\n",
    "# print(\"✅ All frames extracted and saved using a for loop.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727c223-9c57-4979-93ea-6872fbfa2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_folder = \"smile frames\"\n",
    "frames_files = sorted([f for f in os.listdir(frames_folder) if f.endswith('.jpg')] )\n",
    "\n",
    "for frame_file in frames_files:\n",
    "    frame_path = os.path.join(frames_folder, frame_file)\n",
    "    frame = cv2.imread(frame_path)\n",
    "\n",
    "    try:\n",
    "        result = DeepFace.analyze(frame, actions =['emotion'], enforce_detection = False)\n",
    "        emotion = result[0]['dominant_emotion'] if isinstance(result,list) else result['dominant_emotion']\n",
    "        print(f\"{frame_file}: {emotion}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {frame_file}: {e}\")\n",
    "        \n",
    "\n",
    "# There is an ongoing issue with the package - not processing images well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da34b6b-cabb-4d8f-bc04-c3bf6a680d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am creating a new pipeline using different packages to recognise faces and emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a72ca-a489-43c1-ada5-63615026de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating frames for the video \n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to your video\n",
    "video_path = \"/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/resources/basketball.mp4\"\n",
    "frames_folder = \"video_frames\"\n",
    "\n",
    "# Create folder for frames if it doesn't exist\n",
    "if not os.path.exists(frames_folder):\n",
    "    os.makedirs(frames_folder)\n",
    "print(f\"Frames will be saved in: {frames_folder}\")\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Read and save frames\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_filename = os.path.join(frames_folder, f\"frame_{frame_count:04d}.jpg\")\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de45f18-6a29-4e06-ad29-836cdcefdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(cv2.data.haarcascades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b8860-7140-4fe8-9768-07878b601a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# didnt work as expected - the accuracy with which it captured the faces is too low. \n",
    "import cv2\n",
    "import os\n",
    "\n",
    "frames_folder = \"video_frames\"\n",
    "faces_folder = \"cropped_faces\"\n",
    "\n",
    "if not os.path.exists(faces_folder):\n",
    "    os.makedirs(faces_folder)\n",
    "\n",
    "# Load the face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "for frame_name in os.listdir(frames_folder):\n",
    "    frame_path = os.path.join(frames_folder, frame_name)\n",
    "    img = cv2.imread(frame_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # For each detected face, crop and save\n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        out_file = os.path.join(faces_folder, f\"{frame_name[:-4]}_face_{i+1}.jpg\")\n",
    "        cv2.imwrite(out_file, face_img)\n",
    "        print(f\"Saved {out_file}\")\n",
    "\n",
    "print(\"✅ All faces cropped and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d5670-fe0c-484e-9aae-2066cc3db32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "frames_folder = \"video_frames\"\n",
    "faces_folder = \"cropped_faces_mtcnn\"\n",
    "\n",
    "if not os.path.exists(faces_folder):\n",
    "    os.makedirs(faces_folder)\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "for frame_name in os.listdir(frames_folder):\n",
    "    frame_path = os.path.join(frames_folder, frame_name)\n",
    "    img = cv2.imread(frame_path)\n",
    "    results = detector.detect_faces(img)\n",
    "    for i, res in enumerate(results):\n",
    "        x, y, w, h = res['box']\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        out_file = os.path.join(faces_folder, f\"{frame_name[:-4]}_face_{i+1}.jpg\")\n",
    "        cv2.imwrite(out_file, face_img)\n",
    "        print(f\"Saved {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e331a5-29f9-4d12-96a4-08796f36aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import os\n",
    "\n",
    "faces_folder = \"cropped_faces_mtcnn\"  # Or your folder name\n",
    "embeddings = {}  # filename -> embedding\n",
    "\n",
    "for img_name in os.listdir(faces_folder):\n",
    "    img_path = os.path.join(faces_folder, img_name)\n",
    "    try:\n",
    "        # Get embedding vector (using VGG-Face model by default)\n",
    "        embedding = DeepFace.represent(img_path=img_path, model_name='VGG-Face', enforce_detection=False)[0][\"embedding\"]\n",
    "        embeddings[img_name] = embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {img_name}: {e}\")\n",
    "\n",
    "print(\"✅ All embeddings computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6244b-aaa0-42e1-8e46-9828b6d58d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def is_same_person(emb1, emb2, threshold=0.7):\n",
    "    similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    return similarity > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da407d-ffab-4049-aab7-8ac9e61d3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assign a \"person ID\" to each unique face\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "if 'embeddings' not in globals():\n",
    "    raise NameError(\"Variable 'embeddings' is not defined. Please run the cell that defines 'embeddings' first.\")\n",
    "\n",
    "person_id = 1\n",
    "face_to_person = {}\n",
    "person_to_embeddings = defaultdict(list)\n",
    "\n",
    "for img1, emb1 in embeddings.items():\n",
    "    assigned = False\n",
    "    for pid, emb_list in person_to_embeddings.items():\n",
    "        # Check if similar to ANY embedding already in that cluster/person\n",
    "        if any(is_same_person(emb1, e) for e in emb_list):\n",
    "            face_to_person[img1] = pid\n",
    "            person_to_embeddings[pid].append(emb1)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        face_to_person[img1] = person_id\n",
    "        person_to_embeddings[person_id].append(emb1)\n",
    "        person_id += 1\n",
    "\n",
    "print(face_to_person)\n",
    "\n",
    "# Now face_to_person contains mapping of face images to unique person IDs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb5348-2121-43df-890e-3d25e627d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_person(emb1, emb2, threshold=0.5):  # Try 0.5 or even 0.45\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    return cosine_similarity([emb1], [emb2])[0,0] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d6fc1-ba34-4b89-bf4e-0b6ccc63790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "emotions_result = {}\n",
    "\n",
    "for img_file in embeddings.keys():\n",
    "    img_path = f\"cropped_faces_mtcnn/{img_file}\"\n",
    "    try:\n",
    "        result = DeepFace.analyze(img_path=img_path, actions=['emotion'], enforce_detection=False)\n",
    "        # Handle if result is a list (common with DeepFace >=0.0.78)\n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "        emotions_result[img_file] = result['emotion']\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_file}: {e}\")\n",
    "        emotions_result[img_file] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0794fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_emotions = {}\n",
    "\n",
    "for img_file, person_id in face_to_person.items():\n",
    "    emotion = emotions_result.get(img_file)\n",
    "    if emotion is not None:\n",
    "        if person_id not in person_emotions:\n",
    "            person_emotions[person_id] = []\n",
    "        person_emotions[person_id].append({'img_file': img_file, 'emotion': emotion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in person_emotions .items():\n",
    "    person_id, emotions = item\n",
    "    print(f\"Person ID: {person_id}\")\n",
    "    for emotion in emotions:\n",
    "        print(f\"  Image: {emotion['img_file']}, Emotion: {emotion['emotion']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35182d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Step 1: Group all top emotions by person\n",
    "person_emotions = defaultdict(list)\n",
    "\n",
    "for img_file, person_id in face_to_person.items():\n",
    "    emotion_dict = emotions_result.get(img_file)\n",
    "    if emotion_dict:\n",
    "        top_emotion = max(emotion_dict, key=emotion_dict.get)\n",
    "        person_emotions[person_id].append(top_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5affdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for person_id, emotions in person_emotions.items():\n",
    "    if emotions:\n",
    "        # Counter counts how many times each emotion appears\n",
    "        most_common = Counter(emotions).most_common(1)[0]\n",
    "        print(f\"Person {person_id}: Predominant emotion is {most_common[0]} (appeared {most_common[1]} times)\")\n",
    "    else:\n",
    "        print(f\"Person {person_id}: No emotions detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step Workflow for Bounding & Tracking\n",
    "\n",
    "# 1. Detect Faces and Get Bounding Boxes\n",
    "# \t•\tUse a face detector (like MTCNN, RetinaFace, or OpenCV DNN) to detect faces and get bounding boxes for every frame.\n",
    "\n",
    "# 2. Track Faces Across Frames\n",
    "# \t•\tUse a tracker (like SORT, DeepSORT, or even simple IOU matching) to assign a unique ID to each bounding box, so that the same person keeps the same ID as they move frame-to-frame.\n",
    "\n",
    "# 3. Analyze Emotion Within Each Bounding Box\n",
    "# \t•\tFor each bounding box (i.e., each person in each frame), crop that region (just in memory, you don’t need to save to disk) and pass it to your emotion detector (DeepFace, FER, etc).\n",
    "\n",
    "# 4. Aggregate Emotions Per Person (ID)\n",
    "# \t•\tFor each unique ID, collect emotions across frames for stats and visualization.\n",
    "\n",
    "# ⸻\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6016eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "# Load the video\n",
    "video_path = \"/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/resources/basketball.mp4\"\n",
    "\n",
    "output_folder = \"frames_with_boxes\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "detector = MTCNN()\n",
    "frame_num = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    results = detector.detect_faces(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        x, y, width, height = result['box']\n",
    "        cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "    \n",
    "    # Save frame with bounding boxes\n",
    "    frame_filename = os.path.join(output_folder, f\"frame_{frame_num:04d}.jpg\")\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    frame_num += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"✅ All frames with bounding boxes saved in folder: '{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8d292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58366775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# didnt seem to work as expected - the accuracy with which it captured the faces is too low.# I am creating a new pipeline using different packages to recognise faces and emotions\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "\n",
    "model = YOLO('/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/emotion_aware_coach_assistant_practice/yolov8n.pt', task='detect')  # Load the YOLO model for face detection\n",
    "tracker = DeepSort(max_age=30)\n",
    "# Create output directory for frames\n",
    "output_dir = \"output_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cap = cv2.VideoCapture(\"/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/resources/basketball.mp4\")\n",
    "frame_num = 0\n",
    "pad = 20  # pixels of padding around face\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "    detections = []\n",
    "    for r in results:\n",
    "        for box, conf, cls in zip(r.boxes.xyxy, r.boxes.conf, r.boxes.cls):\n",
    "            # All detections are faces, so no filtering needed!\n",
    "            x1 = max(int(box[0]) - pad, 0)\n",
    "            y1 = max(int(box[1]) - pad, 0)\n",
    "            x2 = min(int(box[2]) + pad, frame.shape[1])\n",
    "            y2 = min(int(box[3]) + pad, frame.shape[0])\n",
    "            detections.append(([x1, y1, x2 - x1, y2 - y1], float(conf), 'face'))\n",
    "\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"ID:{track_id}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Tracked Faces\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    frame_num += 1\n",
    "\n",
    "\n",
    "# Only save the last frame if it was successfully read\n",
    "if ret and frame is not None:\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"frame_{frame_num:04d}.jpg\"), frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc632b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from retinaface import RetinaFace\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import os\n",
    "\n",
    "# 1. Setup\n",
    "tracker = DeepSort(max_age=40)\n",
    "video_path = \"/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/resources/basketball.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "output_dir = \"output_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "frame_num = 0  # Make sure to increment for each frame\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 2. Face detection\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    detections = []\n",
    "    if isinstance(faces, dict):  # Only if faces were detected\n",
    "        for key in faces:\n",
    "            face = faces[key]\n",
    "            x1, y1, x2, y2 = face['facial_area']\n",
    "            conf = face.get('score', 0.99)  # Some models don't give score, so default to high\n",
    "            detections.append(([x1, y1, x2-x1, y2-y1], conf, \"face\"))\n",
    "\n",
    "    # 3. Tracking\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # 4. Draw boxes and IDs\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"ID:{track_id}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 2)\n",
    "\n",
    "        # (Optional: add emotion detection and write above the box!)\n",
    "\n",
    "    # 5. Save every processed frame\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"frame_{frame_num:04d}.jpg\"), frame)\n",
    "    frame_num += 1\n",
    "\n",
    "    # 6. Display for live feedback (optional)\n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from retinaface import RetinaFace\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "CONF_THRESH = 0.8\n",
    "MIN_FACE_SIZE = 20\n",
    "RESIZE_WIDTH = 640\n",
    "VIDEO_PATH = \"/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/resources/basketball.mp4\"\n",
    "OUTPUT_DIR = \"output_frames\"\n",
    "VIDEO_OUTPUT_PATH = \"output_video.mp4\"\n",
    "\n",
    "# Initialize components\n",
    "tracker = DeepSort(max_age=30, n_init=3)\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Get video properties for output\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "frame_num = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Store original frame for output\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Optimize detection with resizing\n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "    scale = RESIZE_WIDTH / orig_w\n",
    "    new_h = int(orig_h * scale)\n",
    "    frame_resized = cv2.resize(frame, (RESIZE_WIDTH, new_h))\n",
    "\n",
    "    # Enhanced face detection\n",
    "    faces = RetinaFace.detect_faces(frame_resized)\n",
    "    detections = []\n",
    "    \n",
    "    if isinstance(faces, dict):\n",
    "        for key, face in faces.items():\n",
    "            x1, y1, x2, y2 = face['facial_area']\n",
    "            conf = face.get('score', 0.99)\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            \n",
    "            # Apply confidence and size filters\n",
    "            if conf >= CONF_THRESH and w >= MIN_FACE_SIZE and h >= MIN_FACE_SIZE:\n",
    "                detections.append(([x1, y1, w, h], conf, \"face\"))\n",
    "\n",
    "    # Update tracker\n",
    "    tracks = tracker.update_tracks(detections, frame=frame_resized)\n",
    "\n",
    "    # Process tracks and draw on output frame\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "            \n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        \n",
    "        # Scale coordinates to original frame\n",
    "        x1_orig = int(ltrb[0] / scale)\n",
    "        y1_orig = int(ltrb[1] / scale)\n",
    "        x2_orig = int(ltrb[2] / scale)\n",
    "        y2_orig = int(ltrb[3] / scale)\n",
    "        \n",
    "        # Visualization\n",
    "        cv2.rectangle(output_frame, (x1_orig, y1_orig), (x2_orig, y2_orig), (0,255,0), 2)\n",
    "        cv2.putText(output_frame, f\"ID:{track_id}\", (x1_orig, y1_orig-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 2)\n",
    "\n",
    "    # Save processed frame to folder\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"frame_{frame_num:04d}.jpg\"), output_frame)\n",
    "    \n",
    "    # Write frame to output video\n",
    "    out_video.write(output_frame)\n",
    "    \n",
    "    # Display live feed\n",
    "    cv2.imshow(\"Face Tracking\", output_frame)\n",
    "    frame_num += 1\n",
    "    \n",
    "    # Exit on 'q' press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Processing complete! Saved {frame_num} frames to '{OUTPUT_DIR}' and video to '{VIDEO_OUTPUT_PATH}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad94bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Saved 322 frames to 'output_frames' and video to 'output_video.mp4'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "CONF_THRESH = 0.4      # Lower threshold to catch more faces\n",
    "MIN_FACE_SIZE = 20     # Minimum face width/height in pixels\n",
    "RESIZE_WIDTH = 640     # Resize width for faster detection\n",
    "OUTPUT_DIR = \"output_frames\"\n",
    "VIDEO_OUTPUT_PATH = \"output_video.mp4\"\n",
    "VIDEO_PATH = \"/Users/bujjigaadu/Desktop/Data Analytics projects /Internship/resources/basketball.mp4\"\n",
    "\n",
    "# Initialize\n",
    "tracker = DeepSort(max_age=30, n_init=3)\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Get video properties for output\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "frame_num = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Store original frame for output\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Preprocess: increase contrast and sharpen\n",
    "    frame = cv2.convertScaleAbs(frame, alpha=1.2, beta=20)\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    frame = cv2.filter2D(frame, -1, kernel)\n",
    "\n",
    "    # Resize for faster detection\n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "    scale = RESIZE_WIDTH / orig_w\n",
    "    new_h = int(orig_h * scale)\n",
    "    frame_resized = cv2.resize(frame, (RESIZE_WIDTH, new_h))\n",
    "\n",
    "    # Detect faces with resnet50 backbone\n",
    "    model = RetinaFace.build_model()  # or \"mobilenet\"\n",
    "    faces = RetinaFace.detect_faces(frame_resized, model=model)\n",
    "    detections = []\n",
    "\n",
    "    if isinstance(faces, dict):\n",
    "        for key, face in faces.items():\n",
    "            conf = face.get('score', 0.99)\n",
    "            x1, y1, x2, y2 = face['facial_area']\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            # Optional: filter by region of interest (ROI)\n",
    "            # e.g., only faces in the court area\n",
    "            # court_area = (0, 100, frame_resized.shape[1], frame_resized.shape[0]-50)\n",
    "            # center_x, center_y = (x1 + x2)//2, (y1 + y2)//2\n",
    "            # if (court_area[0] <= center_x <= court_area[2] and\n",
    "            #     court_area[1] <= center_y <= court_area[3]):\n",
    "\n",
    "            # Filter by confidence and size\n",
    "            if conf > CONF_THRESH and w > MIN_FACE_SIZE and h > MIN_FACE_SIZE:\n",
    "                detections.append(([x1, y1, w, h], conf, \"face\"))\n",
    "\n",
    "    # Update tracker with detections\n",
    "    tracks = tracker.update_tracks(detections, frame=frame_resized)\n",
    "\n",
    "    # Draw tracks on original frame (scaled coordinates)\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x1_orig = int(ltrb[0] / scale)\n",
    "        y1_orig = int(ltrb[1] / scale)\n",
    "        x2_orig = int(ltrb[2] / scale)\n",
    "        y2_orig = int(ltrb[3] / scale)\n",
    "        cv2.rectangle(output_frame, (x1_orig, y1_orig), (x2_orig, y2_orig), (0, 255, 0), 2)\n",
    "        cv2.putText(output_frame, f\"ID:{track_id}\", (x1_orig, y1_orig-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "    # Save processed frame and write to video\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"frame_{frame_num:04d}.jpg\"), output_frame)\n",
    "    out_video.write(output_frame)\n",
    "    frame_num += 1\n",
    "\n",
    "    # Display live\n",
    "    cv2.imshow(\"Face Tracking\", output_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Processing complete! Saved {frame_num} frames to '{OUTPUT_DIR}' and video to '{VIDEO_OUTPUT_PATH}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
